{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neal\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# ff = h5py.File(\"raw_chess_data.hdf5\", \"r\")\n",
    "\n",
    "# whiteWins2 = ff['white']\n",
    "# blackWins2 = ff['black']\n",
    "# with h5py.File('raw_chess_data.hdf5','r') as f:\n",
    "#     whiteWins2 = f['white']\n",
    "#     blackWins2 = f['black']\n",
    "#     whiteWins = np.random.permutation(whiteWins2[100000:])[:100000]\n",
    "#     blackWins = np.random.permutation(blackWins2[100000:])[:100000]\n",
    "\n",
    "x = np.random.rand(500,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "500/500 [==============================] - 0s 866us/step - loss: 0.7621 - mean_squared_error: 0.7621\n",
      "Epoch 2/70\n",
      "500/500 [==============================] - 0s 134us/step - loss: 0.3627 - mean_squared_error: 0.3627\n",
      "Epoch 3/70\n",
      "500/500 [==============================] - 0s 104us/step - loss: 0.1919 - mean_squared_error: 0.1919\n",
      "Epoch 4/70\n",
      "500/500 [==============================] - 0s 97us/step - loss: 0.1533 - mean_squared_error: 0.1533\n",
      "Epoch 5/70\n",
      "500/500 [==============================] - 0s 107us/step - loss: 0.1408 - mean_squared_error: 0.1408\n",
      "Epoch 6/70\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.1304 - mean_squared_error: 0.1304\n",
      "Epoch 7/70\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.1219 - mean_squared_error: 0.1219\n",
      "Epoch 8/70\n",
      "500/500 [==============================] - 0s 107us/step - loss: 0.1149 - mean_squared_error: 0.1149\n",
      "Epoch 9/70\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.1088 - mean_squared_error: 0.1088\n",
      "Epoch 10/70\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.1036 - mean_squared_error: 0.1036\n",
      "Epoch 11/70\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0990 - mean_squared_error: 0.0990\n",
      "Epoch 12/70\n",
      "500/500 [==============================] - 0s 107us/step - loss: 0.0952 - mean_squared_error: 0.0952\n",
      "Epoch 13/70\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0917 - mean_squared_error: 0.0917\n",
      "Epoch 14/70\n",
      "500/500 [==============================] - 0s 109us/step - loss: 0.0887 - mean_squared_error: 0.0887\n",
      "Epoch 15/70\n",
      "500/500 [==============================] - 0s 109us/step - loss: 0.0860 - mean_squared_error: 0.0860\n",
      "Epoch 16/70\n",
      "500/500 [==============================] - 0s 105us/step - loss: 0.0834 - mean_squared_error: 0.0834\n",
      "Epoch 17/70\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0811 - mean_squared_error: 0.0811\n",
      "Epoch 18/70\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.0790 - mean_squared_error: 0.0790\n",
      "Epoch 19/70\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.0770 - mean_squared_error: 0.0770\n",
      "Epoch 20/70\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.0752 - mean_squared_error: 0.0752\n",
      "Epoch 21/70\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.0734 - mean_squared_error: 0.0734\n",
      "Epoch 22/70\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0718 - mean_squared_error: 0.0718\n",
      "Epoch 23/70\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.0703 - mean_squared_error: 0.0703\n",
      "Epoch 24/70\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.0688 - mean_squared_error: 0.0688\n",
      "Epoch 25/70\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0675 - mean_squared_error: 0.0675\n",
      "Epoch 26/70\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.0662 - mean_squared_error: 0.0662\n",
      "Epoch 27/70\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0650 - mean_squared_error: 0.0650\n",
      "Epoch 28/70\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.0638 - mean_squared_error: 0.0638\n",
      "Epoch 29/70\n",
      "500/500 [==============================] - 0s 104us/step - loss: 0.0627 - mean_squared_error: 0.0627\n",
      "Epoch 30/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0615 - mean_squared_error: 0.0615\n",
      "Epoch 31/70\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.0605 - mean_squared_error: 0.0605\n",
      "Epoch 32/70\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0596 - mean_squared_error: 0.0596\n",
      "Epoch 33/70\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Epoch 34/70\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.0577 - mean_squared_error: 0.0577\n",
      "Epoch 35/70\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0568 - mean_squared_error: 0.0568\n",
      "Epoch 36/70\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.0559 - mean_squared_error: 0.0559\n",
      "Epoch 37/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0552 - mean_squared_error: 0.0552\n",
      "Epoch 38/70\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.0545 - mean_squared_error: 0.0545\n",
      "Epoch 39/70\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.0537 - mean_squared_error: 0.0537\n",
      "Epoch 40/70\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.0530 - mean_squared_error: 0.0530\n",
      "Epoch 41/70\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.0523 - mean_squared_error: 0.0523\n",
      "Epoch 42/70\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.0517 - mean_squared_error: 0.0517\n",
      "Epoch 43/70\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0510 - mean_squared_error: 0.0510\n",
      "Epoch 44/70\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0504 - mean_squared_error: 0.0504\n",
      "Epoch 45/70\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0498 - mean_squared_error: 0.0498\n",
      "Epoch 46/70\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.0493 - mean_squared_error: 0.0493\n",
      "Epoch 47/70\n",
      "500/500 [==============================] - 0s 82us/step - loss: 0.0486 - mean_squared_error: 0.0486\n",
      "Epoch 48/70\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.0481 - mean_squared_error: 0.0481\n",
      "Epoch 49/70\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0476 - mean_squared_error: 0.0476\n",
      "Epoch 50/70\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 51/70\n",
      "500/500 [==============================] - 0s 79us/step - loss: 0.0467 - mean_squared_error: 0.0467\n",
      "Epoch 52/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0461 - mean_squared_error: 0.0461\n",
      "Epoch 53/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0456 - mean_squared_error: 0.0456\n",
      "Epoch 54/70\n",
      "500/500 [==============================] - 0s 83us/step - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 55/70\n",
      "500/500 [==============================] - 0s 79us/step - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 56/70\n",
      "500/500 [==============================] - 0s 81us/step - loss: 0.0443 - mean_squared_error: 0.0443\n",
      "Epoch 57/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0439 - mean_squared_error: 0.0439\n",
      "Epoch 58/70\n",
      "500/500 [==============================] - 0s 73us/step - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 59/70\n",
      "500/500 [==============================] - 0s 84us/step - loss: 0.0431 - mean_squared_error: 0.0431\n",
      "Epoch 60/70\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 61/70\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 62/70\n",
      "500/500 [==============================] - 0s 83us/step - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 63/70\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0415 - mean_squared_error: 0.0415\n",
      "Epoch 64/70\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 65/70\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 66/70\n",
      "500/500 [==============================] - 0s 87us/step - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 67/70\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 68/70\n",
      "500/500 [==============================] - 0s 79us/step - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 69/70\n",
      "500/500 [==============================] - 0s 80us/step - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 70/70\n",
      "500/500 [==============================] - 0s 89us/step - loss: 0.0392 - mean_squared_error: 0.0392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18fec056978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# training an autoencoder:\n",
    "input_dim = x.shape[1]\n",
    "a1 = 30\n",
    "a2 = 10\n",
    "\n",
    "# first autoencoder layer: input x1, compress to a1 dimensions, expand back to original values \n",
    "inputs = Input(shape=(x.shape[1],))\n",
    "l1 = Dense(a1, activation = 'selu')(inputs)\n",
    "out = Dense(x.shape[1], activation = 'selu')(l1)\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "          loss='mse', metrics=['mse'])\n",
    "# need to give x as the inputs and outputs\n",
    "model.fit(x, x, epochs=70, \n",
    "          batch_size = 32,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "500/500 [==============================] - 0s 891us/step - loss: 0.9934 - mean_squared_error: 0.9934\n",
      "Epoch 2/70\n",
      "500/500 [==============================] - 0s 121us/step - loss: 0.5384 - mean_squared_error: 0.5384\n",
      "Epoch 3/70\n",
      "500/500 [==============================] - 0s 112us/step - loss: 0.3503 - mean_squared_error: 0.3503\n",
      "Epoch 4/70\n",
      "500/500 [==============================] - 0s 131us/step - loss: 0.2444 - mean_squared_error: 0.2444\n",
      "Epoch 5/70\n",
      "500/500 [==============================] - 0s 124us/step - loss: 0.1748 - mean_squared_error: 0.1748\n",
      "Epoch 6/70\n",
      "500/500 [==============================] - 0s 105us/step - loss: 0.1333 - mean_squared_error: 0.1333\n",
      "Epoch 7/70\n",
      "500/500 [==============================] - 0s 115us/step - loss: 0.1128 - mean_squared_error: 0.1128\n",
      "Epoch 8/70\n",
      "500/500 [==============================] - 0s 111us/step - loss: 0.1032 - mean_squared_error: 0.1032\n",
      "Epoch 9/70\n",
      "500/500 [==============================] - 0s 109us/step - loss: 0.0983 - mean_squared_error: 0.0983\n",
      "Epoch 10/70\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0946 - mean_squared_error: 0.0946\n",
      "Epoch 11/70\n",
      "500/500 [==============================] - 0s 104us/step - loss: 0.0912 - mean_squared_error: 0.0912\n",
      "Epoch 12/70\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0887 - mean_squared_error: 0.0887\n",
      "Epoch 13/70\n",
      "500/500 [==============================] - 0s 97us/step - loss: 0.0863 - mean_squared_error: 0.0863\n",
      "Epoch 14/70\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.0843 - mean_squared_error: 0.0843\n",
      "Epoch 15/70\n",
      "500/500 [==============================] - 0s 104us/step - loss: 0.0826 - mean_squared_error: 0.0826\n",
      "Epoch 16/70\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0808 - mean_squared_error: 0.0808\n",
      "Epoch 17/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0794 - mean_squared_error: 0.0794\n",
      "Epoch 18/70\n",
      "500/500 [==============================] - 0s 97us/step - loss: 0.0781 - mean_squared_error: 0.0781\n",
      "Epoch 19/70\n",
      "500/500 [==============================] - 0s 105us/step - loss: 0.0768 - mean_squared_error: 0.0768\n",
      "Epoch 20/70\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0757 - mean_squared_error: 0.0757\n",
      "Epoch 21/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0745 - mean_squared_error: 0.0745\n",
      "Epoch 22/70\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0735 - mean_squared_error: 0.0735\n",
      "Epoch 23/70\n",
      "500/500 [==============================] - 0s 91us/step - loss: 0.0725 - mean_squared_error: 0.0725\n",
      "Epoch 24/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0715 - mean_squared_error: 0.0715\n",
      "Epoch 25/70\n",
      "500/500 [==============================] - 0s 97us/step - loss: 0.0707 - mean_squared_error: 0.0707\n",
      "Epoch 26/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0699 - mean_squared_error: 0.0699\n",
      "Epoch 27/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0691 - mean_squared_error: 0.0691\n",
      "Epoch 28/70\n",
      "500/500 [==============================] - 0s 109us/step - loss: 0.0682 - mean_squared_error: 0.0682\n",
      "Epoch 29/70\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0675 - mean_squared_error: 0.0675\n",
      "Epoch 30/70\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0668 - mean_squared_error: 0.0668\n",
      "Epoch 31/70\n",
      "500/500 [==============================] - 0s 116us/step - loss: 0.0660 - mean_squared_error: 0.0660\n",
      "Epoch 32/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0653 - mean_squared_error: 0.0653\n",
      "Epoch 33/70\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0646 - mean_squared_error: 0.0646\n",
      "Epoch 34/70\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.0640 - mean_squared_error: 0.0640\n",
      "Epoch 35/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0633 - mean_squared_error: 0.0633\n",
      "Epoch 36/70\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.0627 - mean_squared_error: 0.0627\n",
      "Epoch 37/70\n",
      "500/500 [==============================] - 0s 85us/step - loss: 0.0621 - mean_squared_error: 0.0621\n",
      "Epoch 38/70\n",
      "500/500 [==============================] - 0s 113us/step - loss: 0.0615 - mean_squared_error: 0.0615\n",
      "Epoch 39/70\n",
      "500/500 [==============================] - 0s 110us/step - loss: 0.0610 - mean_squared_error: 0.0610\n",
      "Epoch 40/70\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0604 - mean_squared_error: 0.0604\n",
      "Epoch 41/70\n",
      "500/500 [==============================] - 0s 115us/step - loss: 0.0599 - mean_squared_error: 0.0599\n",
      "Epoch 42/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0593 - mean_squared_error: 0.0593\n",
      "Epoch 43/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0588 - mean_squared_error: 0.0588\n",
      "Epoch 44/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0583 - mean_squared_error: 0.0583\n",
      "Epoch 45/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0578 - mean_squared_error: 0.0578\n",
      "Epoch 46/70\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0573 - mean_squared_error: 0.0573\n",
      "Epoch 47/70\n",
      "500/500 [==============================] - 0s 106us/step - loss: 0.0569 - mean_squared_error: 0.0569\n",
      "Epoch 48/70\n",
      "500/500 [==============================] - 0s 88us/step - loss: 0.0565 - mean_squared_error: 0.0565\n",
      "Epoch 49/70\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0560 - mean_squared_error: 0.0560\n",
      "Epoch 50/70\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0556 - mean_squared_error: 0.0556\n",
      "Epoch 51/70\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0552 - mean_squared_error: 0.0552\n",
      "Epoch 52/70\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.0549 - mean_squared_error: 0.0549\n",
      "Epoch 53/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0545 - mean_squared_error: 0.0545\n",
      "Epoch 54/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0541 - mean_squared_error: 0.0541\n",
      "Epoch 55/70\n",
      "500/500 [==============================] - 0s 111us/step - loss: 0.0537 - mean_squared_error: 0.0537\n",
      "Epoch 56/70\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0533 - mean_squared_error: 0.0533\n",
      "Epoch 57/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0530 - mean_squared_error: 0.0530\n",
      "Epoch 58/70\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0526 - mean_squared_error: 0.0526\n",
      "Epoch 59/70\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0523 - mean_squared_error: 0.0523\n",
      "Epoch 60/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0520 - mean_squared_error: 0.0520\n",
      "Epoch 61/70\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.0517 - mean_squared_error: 0.0517\n",
      "Epoch 62/70\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.0514 - mean_squared_error: 0.0514\n",
      "Epoch 63/70\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0511 - mean_squared_error: 0.0511\n",
      "Epoch 64/70\n",
      "500/500 [==============================] - 0s 90us/step - loss: 0.0508 - mean_squared_error: 0.0508\n",
      "Epoch 65/70\n",
      "500/500 [==============================] - 0s 86us/step - loss: 0.0505 - mean_squared_error: 0.0505\n",
      "Epoch 66/70\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0502 - mean_squared_error: 0.0502\n",
      "Epoch 67/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0499 - mean_squared_error: 0.0499\n",
      "Epoch 68/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0497 - mean_squared_error: 0.0497\n",
      "Epoch 69/70\n",
      "500/500 [==============================] - 0s 97us/step - loss: 0.0494 - mean_squared_error: 0.0494\n",
      "Epoch 70/70\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0491 - mean_squared_error: 0.0491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1914d149f60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the second layer of the autoencoder\n",
    "# this is where the functional API comes in handy - we can actually just call l1 again and \n",
    "# get the already trained weights, but if we don't want to keep\n",
    "# training it, we can just pull the weights out instead and set trainable = false\n",
    "\n",
    "# first, we need the outputs:\n",
    "m2 = Model(inputs=inputs, outputs=l1)\n",
    "m2.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "x2 = m2.predict(x)\n",
    "\n",
    "# next, we can build another layer on top of l1, but that will cause l1 to continue to train, so it may not be the best:\n",
    "l2 = Dense(a2, activation = 'selu')(l1)\n",
    "out = Dense(a1, activation = 'selu')(l2)\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse', metrics=['mse'])\n",
    "model.fit(x, x2, epochs=70, \n",
    "          batch_size = 32,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2370961   0.40166467 -0.62386763 -0.74518055 -0.27684033  0.48909643\n",
      " -1.2642627   0.5088347  -1.2736311  -1.4862788   0.44073248 -1.2403533\n",
      "  0.5399281  -0.37873524 -1.4256694  -0.8731435   0.9555403  -1.1208768\n",
      "  0.39457324  0.15743172 -0.7520019   1.669019    1.8687142  -0.39997151\n",
      " -0.20438246  0.06255385  0.34523028 -0.47740695 -0.6081827   0.3524968 ] [-1.1070328  -0.14766712 -1.0270394  -0.8163393  -1.2066611   1.9432664\n",
      " -1.3438878   0.8914418  -1.1711365  -1.4491794   0.76663774 -1.3006308\n",
      "  0.32917118  0.35923654 -1.0142576  -1.3809918   1.3902638  -0.94833666\n",
      "  1.0981576   1.2987351   0.20346548  1.3478832   1.5440772  -0.67308426\n",
      " -1.3389326   1.101044    0.7584531  -0.56174517 -1.2142802   0.7316164 ]\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can extract the weights from l1 (or from the model) and set them into a new layer that's frozen:\n",
    "x3 = m2.predict(x)\n",
    "print(x2[0], x3[0])\n",
    "# we can see that l1 has been trained even in the new model. Sort of neat, but not necessarily ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1070328  -0.14766712 -1.0270394  -0.8163393  -1.2066611   1.9432664\n",
      " -1.3438878   0.8914418  -1.1711365  -1.4491794   0.76663774 -1.3006308\n",
      "  0.32917118  0.35923654 -1.0142576  -1.3809918   1.3902638  -0.94833666\n",
      "  1.0981576   1.2987351   0.20346548  1.3478832   1.5440772  -0.67308426\n",
      " -1.3389326   1.101044    0.7584531  -0.56174517 -1.2142802   0.7316164 ] [-1.1070328  -0.14766712 -1.0270394  -0.8163393  -1.2066611   1.9432664\n",
      " -1.3438878   0.8914418  -1.1711365  -1.4491794   0.76663774 -1.3006308\n",
      "  0.32917118  0.35923654 -1.0142576  -1.3809918   1.3902638  -0.94833666\n",
      "  1.0981576   1.2987351   0.20346548  1.3478832   1.5440772  -0.67308426\n",
      " -1.3389326   1.101044    0.7584531  -0.56174517 -1.2142802   0.7316164 ]\n"
     ]
    }
   ],
   "source": [
    "# we can try to set the layers to frozen, though. let's keep it how it is right now, freeze the l1 layer, and repeat the above steps:\n",
    "for l in m2.layers: \n",
    "    l.trainable=False\n",
    "l2 = Dense(a2, activation = 'selu')(l1)\n",
    "out = Dense(a1, activation = 'selu')(l2)\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse', metrics=['mse'])\n",
    "model.fit(x, x3, epochs=70, \n",
    "          batch_size = 32,\n",
    "          verbose=0)\n",
    "x4 = m2.predict(x)\n",
    "print(x3[0], x4[0])\n",
    "# sweeeeeeet it works! \n",
    "# typically, you don't want to continue training an autoencoder anyway, so this is perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1915b327ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so, to add another layer, we'd do the same as before:\n",
    "# we need the outputs again:\n",
    "m3 = Model(inputs=inputs, outputs=l2)\n",
    "m3.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "x5 = m3.predict(x)\n",
    "for l in m3.layers: \n",
    "    l.trainable=False\n",
    "\n",
    "# we can build another layer on top of l2\n",
    "a3 = 5\n",
    "l3 = Dense(a3, activation = 'selu')(l2)\n",
    "out = Dense(a2, activation = 'selu')(l3)\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse', metrics=['mse'])\n",
    "model.fit(x, x5, epochs=70, \n",
    "          batch_size = 32,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 1.3307\n",
      "Epoch 2/70\n",
      "500/500 [==============================] - 0s 106us/step - loss: 0.8427\n",
      "Epoch 3/70\n",
      "500/500 [==============================] - 0s 116us/step - loss: 0.5186\n",
      "Epoch 4/70\n",
      "500/500 [==============================] - 0s 112us/step - loss: 0.3435\n",
      "Epoch 5/70\n",
      "500/500 [==============================] - 0s 107us/step - loss: 0.2508\n",
      "Epoch 6/70\n",
      "500/500 [==============================] - 0s 119us/step - loss: 0.1882\n",
      "Epoch 7/70\n",
      "500/500 [==============================] - 0s 119us/step - loss: 0.1595\n",
      "Epoch 8/70\n",
      "500/500 [==============================] - 0s 106us/step - loss: 0.1352\n",
      "Epoch 9/70\n",
      "500/500 [==============================] - 0s 129us/step - loss: 0.1294\n",
      "Epoch 10/70\n",
      "500/500 [==============================] - 0s 105us/step - loss: 0.1246\n",
      "Epoch 11/70\n",
      "500/500 [==============================] - 0s 114us/step - loss: 0.1101\n",
      "Epoch 12/70\n",
      "500/500 [==============================] - 0s 109us/step - loss: 0.1092\n",
      "Epoch 13/70\n",
      "500/500 [==============================] - 0s 112us/step - loss: 0.1018\n",
      "Epoch 14/70\n",
      "500/500 [==============================] - 0s 93us/step - loss: 0.1007\n",
      "Epoch 15/70\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0981\n",
      "Epoch 16/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0990\n",
      "Epoch 17/70\n",
      "500/500 [==============================] - 0s 106us/step - loss: 0.1010\n",
      "Epoch 18/70\n",
      "500/500 [==============================] - 0s 118us/step - loss: 0.0979\n",
      "Epoch 19/70\n",
      "500/500 [==============================] - 0s 116us/step - loss: 0.0960\n",
      "Epoch 20/70\n",
      "500/500 [==============================] - 0s 113us/step - loss: 0.0949\n",
      "Epoch 21/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0943\n",
      "Epoch 22/70\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0957\n",
      "Epoch 23/70\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0926\n",
      "Epoch 24/70\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0944\n",
      "Epoch 25/70\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0905\n",
      "Epoch 26/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0902\n",
      "Epoch 27/70\n",
      "500/500 [==============================] - 0s 105us/step - loss: 0.0884\n",
      "Epoch 28/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0890\n",
      "Epoch 29/70\n",
      "500/500 [==============================] - 0s 96us/step - loss: 0.0924\n",
      "Epoch 30/70\n",
      "500/500 [==============================] - 0s 104us/step - loss: 0.0884\n",
      "Epoch 31/70\n",
      "500/500 [==============================] - 0s 104us/step - loss: 0.0906\n",
      "Epoch 32/70\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0913\n",
      "Epoch 33/70\n",
      "500/500 [==============================] - 0s 113us/step - loss: 0.0913\n",
      "Epoch 34/70\n",
      "500/500 [==============================] - 0s 111us/step - loss: 0.0909\n",
      "Epoch 35/70\n",
      "500/500 [==============================] - 0s 129us/step - loss: 0.0886\n",
      "Epoch 36/70\n",
      "500/500 [==============================] - 0s 106us/step - loss: 0.0891\n",
      "Epoch 37/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0873\n",
      "Epoch 38/70\n",
      "500/500 [==============================] - 0s 110us/step - loss: 0.0897\n",
      "Epoch 39/70\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0886\n",
      "Epoch 40/70\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0914\n",
      "Epoch 41/70\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0911\n",
      "Epoch 42/70\n",
      "500/500 [==============================] - 0s 104us/step - loss: 0.0906\n",
      "Epoch 43/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0905\n",
      "Epoch 44/70\n",
      "500/500 [==============================] - 0s 111us/step - loss: 0.0888\n",
      "Epoch 45/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0894\n",
      "Epoch 46/70\n",
      "500/500 [==============================] - 0s 95us/step - loss: 0.0892\n",
      "Epoch 47/70\n",
      "500/500 [==============================] - 0s 116us/step - loss: 0.0881\n",
      "Epoch 48/70\n",
      "500/500 [==============================] - 0s 100us/step - loss: 0.0880\n",
      "Epoch 49/70\n",
      "500/500 [==============================] - 0s 103us/step - loss: 0.0896\n",
      "Epoch 50/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0897\n",
      "Epoch 51/70\n",
      "500/500 [==============================] - 0s 119us/step - loss: 0.0860\n",
      "Epoch 52/70\n",
      "500/500 [==============================] - 0s 111us/step - loss: 0.0866\n",
      "Epoch 53/70\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0884\n",
      "Epoch 54/70\n",
      "500/500 [==============================] - 0s 121us/step - loss: 0.0873\n",
      "Epoch 55/70\n",
      "500/500 [==============================] - 0s 101us/step - loss: 0.0848\n",
      "Epoch 56/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0856\n",
      "Epoch 57/70\n",
      "500/500 [==============================] - 0s 107us/step - loss: 0.0871\n",
      "Epoch 58/70\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0869\n",
      "Epoch 59/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0873\n",
      "Epoch 60/70\n",
      "500/500 [==============================] - 0s 98us/step - loss: 0.0873\n",
      "Epoch 61/70\n",
      "500/500 [==============================] - 0s 106us/step - loss: 0.0892\n",
      "Epoch 62/70\n",
      "500/500 [==============================] - 0s 114us/step - loss: 0.0862\n",
      "Epoch 63/70\n",
      "500/500 [==============================] - 0s 97us/step - loss: 0.0882\n",
      "Epoch 64/70\n",
      "500/500 [==============================] - 0s 92us/step - loss: 0.0883\n",
      "Epoch 65/70\n",
      "500/500 [==============================] - 0s 94us/step - loss: 0.0878\n",
      "Epoch 66/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0896\n",
      "Epoch 67/70\n",
      "500/500 [==============================] - 0s 102us/step - loss: 0.0879\n",
      "Epoch 68/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0864\n",
      "Epoch 69/70\n",
      "500/500 [==============================] - 0s 108us/step - loss: 0.0859\n",
      "Epoch 70/70\n",
      "500/500 [==============================] - 0s 99us/step - loss: 0.0889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1915b8b78d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once that's done, to get the autoencoder, we create a model and freeze the layers:\n",
    "m4 = Model(inputs=inputs, outputs=l3)\n",
    "m4.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "for l in m4.layers: \n",
    "    l.trainable=False\n",
    "    \n",
    "# then we use the functional API to build more network after it:\n",
    "# if we are using Selu, we should use AlphaDropout ( it's explained in the paper cited in keras documentation, basically when a neuron is off from dropout\n",
    "# the value it predict should be 0, but using normal dropout the value it gives is -alpha, so they made a special version\n",
    "# of dropout that corrects for this.)\n",
    "from keras.layers import AlphaDropout\n",
    "y = np.random.rand(500,2)\n",
    "deep1 = Dense(256, activation = 'selu')(l3)\n",
    "deep2 = Dense(256, activation = 'selu')(deep1)\n",
    "drop2 = AlphaDropout(0.5)(deep2)\n",
    "out = Dense(y.shape[1], activation='linear')(drop2)\n",
    "m5 = Model(inputs=inputs, outputs=out)\n",
    "m5.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "m5.fit(x, y, epochs=70, \n",
    "          batch_size = 32,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8927445 1.0677538] [0.0176113  0.90055596]\n"
     ]
    }
   ],
   "source": [
    "pred = m5.predict(x)\n",
    "print(pred[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
