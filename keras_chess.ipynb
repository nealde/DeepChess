{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neal\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2598560, 769)\n",
      "(1925000, 769)\n"
     ]
    }
   ],
   "source": [
    "# x = np.random.randn(100,769) # simulated chess data\n",
    "# y = np.random.randint(0,2,100) # random board state\n",
    "def getTest(whitewins, blackwins):\n",
    "    test = []\n",
    "    test_l = []\n",
    "    for i in range(whitewins.shape[0]):\n",
    "        if np.random.randn(1)>0.5:\n",
    "            test.append([whitewins[i],blackwins[i]])\n",
    "            test_l.append([1, 0])\n",
    "        else:\n",
    "            test.append([blackwins[i],whitewins[i]])\n",
    "            test_l.append([0, 1])\n",
    "#         temp_test = np.zeros((len(test), 2, input_size))\n",
    "#     for i in range(len(test)):\n",
    "#         first = bitifyFEN(test[i][0])\t\n",
    "#         second = bitifyFEN(test[i][1])\t\n",
    "#         elem = [first,second]\n",
    "#         temp_test[i] = elem\n",
    "    return (np.array(test), np.array(test_l))\n",
    "\n",
    "def getBatchAE(start, size):\n",
    "    global whiteWins \n",
    "    global blackWins \n",
    "    size = size//2\n",
    "    start = start*size\n",
    "\n",
    "    temp1 = np.concatenate((whiteWins[start:start+size],blackWins[start:start+size]),axis=0)\n",
    "    np.random.shuffle(temp1)\n",
    "    return temp1\n",
    "\n",
    "# ff = h5py.File(\"raw_chess_data.hdf5\", \"r\")\n",
    "with h5py.File(\"raw_chess_data.hdf5\", 'r') as ff:\n",
    "# print(ff.keys())\n",
    "    whiteWins2 = ff['white'][:]\n",
    "    blackWins2 = ff['black'][:]\n",
    "# validation_test, validation_test_l = getTest(whiteWins2[:100000],blackWins2[:100000])\n",
    "print(whiteWins2.shape)\n",
    "print(blackWins2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatchMLP(start, size):\n",
    "    import random\n",
    "    global whiteWins \n",
    "    global blackWins \n",
    "\n",
    "    xR = []\n",
    "    lR = []\n",
    "\n",
    "    for i in range(start,start+size):\n",
    "        if random.random() > 0.5:\n",
    "            elem = [whiteWins[i], blackWins[i]]\n",
    "            elem_l = [1,0]\n",
    "        else:\n",
    "            elem = [blackWins[i], whiteWins[i]]\n",
    "            elem_l = [0,1]\n",
    "        xR.append(elem)\n",
    "        lR.append(elem_l)\n",
    "    return (np.array(xR), np.array(lR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create initial filters to train individually:\n",
    "# print(y)\n",
    "whiteWins = np.random.permutation(whiteWins2[100000:])[:100000]\n",
    "blackWins = np.random.permutation(blackWins2[100000:])[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make whitewins and blackwins\n",
    "# whitewins = x[np.where(y==1)[0]]\n",
    "# blackwins = x[np.where(y==0)[0]]\n",
    "\n",
    "# def shuffle(n):\n",
    "# #     ind1 = np.random.randint(0,whitewins.shape[0],n)\n",
    "# #     ind2 = np.random.randint(0,blackwins.shape[0],n)\n",
    "#     ind1 = np.random.randint(0,whiteWins2.shape[0],n)\n",
    "#     ind2 = np.random.randint(0,blackWins2.shape[0],n)\n",
    "    \n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for i in range(n):\n",
    "#         chance = np.random.rand()\n",
    "#         if chance>0.5:\n",
    "#             x.append([whiteWins2[ind1[i]], blackWins2[ind2[i]]])\n",
    "#             y.append([1,0])\n",
    "#         else:\n",
    "#             x.append([blackWins2[ind2[i]], whiteWins2[ind1[i]]])\n",
    "#             y.append([0,1])\n",
    "# #     x = [whitewins[ind1], bl]\n",
    "#     x = np.array(x)\n",
    "# #     print(x.shape)\n",
    "#     y = np.array(y)\n",
    "#     return [x[:,0],x[:,1]], y\n",
    "# shuffle(30)\n",
    "\n",
    "def shuffle(n):\n",
    "#     ind1 = np.random.randint(0,whitewins.shape[0],n)\n",
    "#     ind2 = np.random.randint(0,blackwins.shape[0],n)\n",
    "    ind1 = sorted(np.random.randint(0,whiteWins2.shape[0],n))\n",
    "    ind2 = sorted(np.random.randint(0,blackWins2.shape[0],n))\n",
    "    \n",
    "    x1 = np.random.permutation(whiteWins2[ind1])\n",
    "    x2 = np.random.permutation(blackWins2[ind2])\n",
    "    \n",
    "#     print(x1.shape, x2.shape)\n",
    "#     print(whiteWins2.shape)\n",
    "#     return\n",
    "    \n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(n):\n",
    "        chance = np.random.rand()\n",
    "        if chance>0.5:\n",
    "            x.append([x1[i], x2[i]])\n",
    "            y.append([1,0])\n",
    "        else:\n",
    "            x.append([x2[i], x1[i]])\n",
    "            y.append([0,1])\n",
    "#     x = [whitewins[ind1], bl]\n",
    "    x = np.array(x)\n",
    "#     print(x.shape)\n",
    "    y = np.array(y)\n",
    "    return [x[:,0],x[:,1]], y\n",
    "# print(whiteWins2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2, 769)\n",
      "(500, 769)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit \n",
    "a, b = shuffle(500)\n",
    "print(a[1:10][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0085 - mean_squared_error: 0.0085\n",
      "Epoch 2/150\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 3/150\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 4/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 5/150\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 6/150\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 7/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 8/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 9/150\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 10/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 11/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 12/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 13/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 14/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 15/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 16/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 17/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 18/150\n",
      "200000/200000 [==============================] - 4s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 19/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 20/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 21/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 22/150\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 23/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 24/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 25/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 26/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 27/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 28/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 29/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 30/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 31/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 32/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 33/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 34/150\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 35/150\n",
      "200000/200000 [==============================] - 4s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 36/150\n",
      "200000/200000 [==============================] - 4s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 37/150\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 38/150\n",
      "200000/200000 [==============================] - 4s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 39/150\n",
      "200000/200000 [==============================] - 4s 22us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 40/150\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 41/150\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 42/150\n",
      " 44032/200000 [=====>........................] - ETA: 3s - loss: 0.0060 - mean_squared_error: 0.0060"
     ]
    }
   ],
   "source": [
    "def train_manual_autoencoder(x, ae_epochs=20):\n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model, Sequential\n",
    "    from keras import backend as K\n",
    "    \n",
    "    a1 = 600\n",
    "    a2 = 400\n",
    "    a3 = 200\n",
    "    a4 = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(a1, input_shape=(x.shape[1],)))\n",
    "    model.add(Dense(a2))\n",
    "    model.add(Dense(a3))\n",
    "    model.add(Dense(a4))\n",
    "    model.add(Dense(x.shape[1]))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse', metrics=['mse'])\n",
    "    #train the model\n",
    "    model.fit(x, x, epochs=ae_epochs, \n",
    "                  batch_size = 256,\n",
    "                  verbose=1)\n",
    "    return model\n",
    "\n",
    "train_manual_autoencoder(np.concatenate((blackWins, whiteWins),axis=0), 150)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[769, 600], [600, 400], [400, 200], [200, 100]]\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 2.2089e-04 - mean_squared_error: 2.2089e-04\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 1.9449e-04 - mean_squared_error: 1.9449e-04\n",
      "(200000, 600)\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0014 - mean_squared_error: 0.0014\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0012 - mean_squared_error: 0.0012\n",
      "(200000, 400)\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0086 - mean_squared_error: 0.0086\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "(200000, 200)\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 3s 13us/step - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0084 - mean_squared_error: 0.0084\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 3s 13us/step - loss: 0.0082 - mean_squared_error: 0.0082\n",
      "(200000, 100)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0d6a0165cbca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblackWins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhiteWins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'autoencoder.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(x, ae_epochs=20):\n",
    "    \n",
    "    from keras.layers import Input, Dense\n",
    "    from keras.models import Model\n",
    "    from keras import backend as K\n",
    "    \n",
    "    a1 = 600\n",
    "    a2 = 400\n",
    "    a3 = 200\n",
    "    a4 = 100\n",
    "    \n",
    "    pairs = [[x.shape[1],a1],[a1,a2],[a2,a3],[a3,a4]]\n",
    "    layer_weights = []\n",
    "    print(pairs)\n",
    "    \n",
    "#     inputs = Input(shape=(x.shape[1],))\n",
    "    for i in pairs:\n",
    "        # train model\n",
    "        inputs = Input(shape=(i[0],))\n",
    "        l1 = Dense(i[1], activation = 'elu')(inputs)\n",
    "        out = Dense(i[0], activation = 'elu')(l1)\n",
    "        model = Model(inputs=inputs, outputs=out)\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss='mse', metrics=['mse'])\n",
    "        model.fit(x, x, epochs=ae_epochs, \n",
    "                  batch_size = 256,\n",
    "                  verbose=1)\n",
    "#         model.evaluate(x, x)\n",
    "        \n",
    "        # extract weights and output intermediate values:\n",
    "        layer_weights.append(model.layers[1].get_weights())\n",
    "        inp = model.input                                           # input placeholder\n",
    "        outputs = [layer.output for layer in model.layers]# all layer outputs\n",
    "        functor = K.function([inp]+ [K.learning_phase()], [outputs[1]] ) # evaluation function\n",
    "        x = functor([x, 1.])[0]\n",
    "        print(x.shape)\n",
    "        \n",
    "        \n",
    "#     print(layer_weights)\n",
    "    # reconstruct the model using the weights:\n",
    "    inputs = Input(shape = (pairs[0][0],))\n",
    "    ae1 = Dense(a1, activation='elu',trainable=False)(inputs)\n",
    "    ae2 = Dense(a2, activation='elu',trainable=False)(ae1)\n",
    "    ae3 = Dense(a3, activation='elu',trainable=False)(ae2)\n",
    "    out = Dense(a4, activation='elu',trainable=False)(ae3)\n",
    "    ae = Model(inputs, out)\n",
    "    count = 0\n",
    "    for layer in ae.layers:\n",
    "        if count>0:\n",
    "            layer.set_weights(layer_weights[count-1])\n",
    "    # need to assign weights - maybe compile the model and save it?\n",
    "#     return inputs, ae1, ae2, ae3, out, layer_weights\n",
    "    return ae\n",
    "#     return layer_weights\n",
    "\n",
    "ae = train_autoencoder(np.concatenate((blackWins, whiteWins),axis=0), 3)\n",
    "ae.save('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neal\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# ae.save('autoencoder.h5')\n",
    "ae = keras.models.load_model('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      " - 21s - loss: 0.2350 - acc: 0.5954\n",
      "Epoch 2/250\n",
      " - 19s - loss: 0.2285 - acc: 0.6161\n",
      "Epoch 3/250\n",
      " - 18s - loss: 0.2249 - acc: 0.6271\n",
      "Epoch 4/250\n",
      " - 18s - loss: 0.2223 - acc: 0.6335\n",
      "Epoch 5/250\n",
      " - 19s - loss: 0.2210 - acc: 0.6371\n",
      "Epoch 6/250\n",
      " - 20s - loss: 0.2198 - acc: 0.6400\n",
      "Epoch 7/250\n",
      " - 18s - loss: 0.2187 - acc: 0.6422\n",
      "Epoch 8/250\n",
      " - 18s - loss: 0.2184 - acc: 0.6430\n",
      "Epoch 9/250\n",
      " - 19s - loss: 0.2172 - acc: 0.6460\n",
      "Epoch 10/250\n",
      " - 18s - loss: 0.2165 - acc: 0.6478\n",
      "Epoch 11/250\n",
      " - 18s - loss: 0.2158 - acc: 0.6501\n",
      "Epoch 12/250\n",
      " - 18s - loss: 0.2152 - acc: 0.6511\n",
      "Epoch 13/250\n",
      " - 19s - loss: 0.2145 - acc: 0.6527\n",
      "Epoch 14/250\n",
      " - 20s - loss: 0.2140 - acc: 0.6542\n",
      "Epoch 15/250\n",
      " - 20s - loss: 0.2133 - acc: 0.6556\n",
      "Epoch 16/250\n",
      " - 19s - loss: 0.2125 - acc: 0.6578\n",
      "Epoch 17/250\n",
      " - 20s - loss: 0.2125 - acc: 0.6573\n",
      "Epoch 18/250\n",
      " - 20s - loss: 0.2117 - acc: 0.6586\n",
      "Epoch 19/250\n",
      " - 20s - loss: 0.2114 - acc: 0.6602\n",
      "Epoch 20/250\n",
      " - 21s - loss: 0.2112 - acc: 0.6605\n",
      "Epoch 21/250\n",
      " - 22s - loss: 0.2105 - acc: 0.6625\n",
      "Epoch 22/250\n",
      " - 22s - loss: 0.2102 - acc: 0.6628\n",
      "Epoch 23/250\n",
      " - 21s - loss: 0.2098 - acc: 0.6635\n",
      "Epoch 24/250\n",
      " - 21s - loss: 0.2095 - acc: 0.6644\n",
      "Epoch 25/250\n",
      " - 20s - loss: 0.2092 - acc: 0.6651\n",
      "Epoch 26/250\n",
      " - 21s - loss: 0.2087 - acc: 0.6666\n",
      "Epoch 27/250\n",
      " - 21s - loss: 0.2081 - acc: 0.6676\n",
      "Epoch 28/250\n",
      " - 21s - loss: 0.2081 - acc: 0.6675\n",
      "Epoch 29/250\n",
      " - 21s - loss: 0.2076 - acc: 0.6687\n",
      "Epoch 30/250\n",
      " - 19s - loss: 0.2074 - acc: 0.6690\n",
      "Epoch 31/250\n",
      " - 19s - loss: 0.2071 - acc: 0.6698\n",
      "Epoch 32/250\n",
      " - 19s - loss: 0.2064 - acc: 0.6712\n",
      "Epoch 33/250\n",
      " - 19s - loss: 0.2060 - acc: 0.6721\n",
      "Epoch 34/250\n",
      " - 18s - loss: 0.2059 - acc: 0.6726\n",
      "Epoch 35/250\n",
      " - 19s - loss: 0.2058 - acc: 0.6727\n",
      "Epoch 36/250\n",
      " - 19s - loss: 0.2056 - acc: 0.6727\n",
      "Epoch 37/250\n",
      " - 18s - loss: 0.2055 - acc: 0.6738\n",
      "Epoch 38/250\n",
      " - 18s - loss: 0.2047 - acc: 0.6752\n",
      "Epoch 39/250\n",
      " - 19s - loss: 0.2045 - acc: 0.6757\n",
      "Epoch 40/250\n",
      " - 19s - loss: 0.2042 - acc: 0.6765\n",
      "Epoch 41/250\n",
      " - 19s - loss: 0.2041 - acc: 0.6762\n",
      "Epoch 42/250\n",
      " - 19s - loss: 0.2037 - acc: 0.6771\n",
      "Epoch 43/250\n",
      " - 19s - loss: 0.2038 - acc: 0.6772\n",
      "Epoch 44/250\n",
      " - 19s - loss: 0.2035 - acc: 0.6778\n",
      "Epoch 45/250\n",
      " - 19s - loss: 0.2034 - acc: 0.6780\n",
      "Epoch 46/250\n",
      " - 19s - loss: 0.2028 - acc: 0.6798\n",
      "Epoch 47/250\n",
      " - 18s - loss: 0.2029 - acc: 0.6787\n",
      "Epoch 48/250\n",
      " - 19s - loss: 0.2029 - acc: 0.6789\n",
      "Epoch 49/250\n",
      " - 19s - loss: 0.2021 - acc: 0.6806\n",
      "Epoch 50/250\n",
      " - 19s - loss: 0.2019 - acc: 0.6805\n",
      "Epoch 51/250\n",
      " - 19s - loss: 0.2023 - acc: 0.6804\n",
      "Epoch 52/250\n",
      " - 20s - loss: 0.2013 - acc: 0.6830\n",
      "Epoch 53/250\n",
      " - 19s - loss: 0.2015 - acc: 0.6822\n",
      "Epoch 54/250\n",
      " - 20s - loss: 0.2015 - acc: 0.6820\n",
      "Epoch 55/250\n",
      " - 19s - loss: 0.2012 - acc: 0.6831\n",
      "Epoch 56/250\n",
      " - 18s - loss: 0.2012 - acc: 0.6826\n",
      "Epoch 57/250\n",
      " - 19s - loss: 0.2008 - acc: 0.6833\n",
      "Epoch 58/250\n",
      " - 18s - loss: 0.2011 - acc: 0.6826\n",
      "Epoch 59/250\n",
      " - 18s - loss: 0.2003 - acc: 0.6845\n",
      "Epoch 60/250\n",
      " - 18s - loss: 0.2003 - acc: 0.6843\n",
      "Epoch 61/250\n",
      " - 19s - loss: 0.1998 - acc: 0.6851\n",
      "Epoch 62/250\n",
      " - 19s - loss: 0.2001 - acc: 0.6850\n",
      "Epoch 63/250\n",
      " - 18s - loss: 0.2001 - acc: 0.6849\n",
      "Epoch 64/250\n",
      " - 19s - loss: 0.1997 - acc: 0.6856\n",
      "Epoch 65/250\n",
      " - 19s - loss: 0.1993 - acc: 0.6869\n",
      "Epoch 66/250\n",
      " - 19s - loss: 0.1997 - acc: 0.6861\n",
      "Epoch 67/250\n",
      " - 19s - loss: 0.1993 - acc: 0.6865\n",
      "Epoch 68/250\n",
      " - 19s - loss: 0.1992 - acc: 0.6870\n",
      "Epoch 69/250\n",
      " - 18s - loss: 0.1991 - acc: 0.6871\n",
      "Epoch 70/250\n",
      " - 19s - loss: 0.1992 - acc: 0.6873\n",
      "Epoch 71/250\n",
      " - 19s - loss: 0.1986 - acc: 0.6881\n",
      "Epoch 72/250\n",
      " - 18s - loss: 0.1986 - acc: 0.6883\n",
      "Epoch 73/250\n",
      " - 20s - loss: 0.1983 - acc: 0.6883\n",
      "Epoch 74/250\n",
      " - 19s - loss: 0.1985 - acc: 0.6883\n",
      "Epoch 75/250\n",
      " - 19s - loss: 0.1980 - acc: 0.6892\n",
      "Epoch 76/250\n",
      " - 19s - loss: 0.1981 - acc: 0.6891\n",
      "Epoch 77/250\n",
      " - 19s - loss: 0.1980 - acc: 0.6900\n",
      "Epoch 78/250\n",
      " - 18s - loss: 0.1979 - acc: 0.6893\n",
      "Epoch 79/250\n",
      " - 19s - loss: 0.1980 - acc: 0.6894\n",
      "Epoch 80/250\n",
      " - 19s - loss: 0.1980 - acc: 0.6897\n",
      "Epoch 81/250\n",
      " - 19s - loss: 0.1976 - acc: 0.6905\n",
      "Epoch 82/250\n",
      " - 18s - loss: 0.1976 - acc: 0.6908\n",
      "Epoch 83/250\n",
      " - 19s - loss: 0.1971 - acc: 0.6911\n",
      "Epoch 84/250\n",
      " - 19s - loss: 0.1973 - acc: 0.6905\n",
      "Epoch 85/250\n",
      " - 19s - loss: 0.1969 - acc: 0.6924\n",
      "Epoch 86/250\n",
      " - 20s - loss: 0.1973 - acc: 0.6907\n",
      "Epoch 87/250\n",
      " - 19s - loss: 0.1970 - acc: 0.6915\n",
      "Epoch 88/250\n",
      " - 20s - loss: 0.1965 - acc: 0.6927\n",
      "Epoch 89/250\n",
      " - 19s - loss: 0.1965 - acc: 0.6924\n",
      "Epoch 90/250\n",
      " - 19s - loss: 0.1969 - acc: 0.6916\n",
      "Epoch 91/250\n",
      " - 18s - loss: 0.1966 - acc: 0.6926\n",
      "Epoch 92/250\n",
      " - 18s - loss: 0.1963 - acc: 0.6932\n",
      "Epoch 93/250\n",
      " - 19s - loss: 0.1963 - acc: 0.6929\n",
      "Epoch 94/250\n",
      " - 19s - loss: 0.1964 - acc: 0.6927\n",
      "Epoch 95/250\n",
      " - 19s - loss: 0.1960 - acc: 0.6933\n",
      "Epoch 96/250\n",
      " - 18s - loss: 0.1960 - acc: 0.6939\n",
      "Epoch 97/250\n",
      " - 19s - loss: 0.1956 - acc: 0.6941\n",
      "Epoch 98/250\n",
      " - 18s - loss: 0.1957 - acc: 0.6947\n",
      "Epoch 99/250\n",
      " - 18s - loss: 0.1961 - acc: 0.6937\n",
      "Epoch 100/250\n",
      " - 18s - loss: 0.1957 - acc: 0.6943\n",
      "Epoch 101/250\n",
      " - 19s - loss: 0.1957 - acc: 0.6943\n",
      "Epoch 102/250\n",
      " - 19s - loss: 0.1953 - acc: 0.6947\n",
      "Epoch 103/250\n",
      " - 18s - loss: 0.1954 - acc: 0.6947\n",
      "Epoch 104/250\n",
      " - 18s - loss: 0.1959 - acc: 0.6937\n",
      "Epoch 105/250\n",
      " - 19s - loss: 0.1954 - acc: 0.6952\n",
      "Epoch 106/250\n",
      " - 18s - loss: 0.1949 - acc: 0.6957\n",
      "Epoch 107/250\n",
      " - 18s - loss: 0.1948 - acc: 0.6961\n",
      "Epoch 108/250\n",
      " - 18s - loss: 0.1952 - acc: 0.6949\n",
      "Epoch 109/250\n",
      " - 19s - loss: 0.1948 - acc: 0.6959\n",
      "Epoch 110/250\n",
      " - 19s - loss: 0.1949 - acc: 0.6958\n",
      "Epoch 111/250\n",
      " - 19s - loss: 0.1947 - acc: 0.6967\n",
      "Epoch 112/250\n",
      " - 19s - loss: 0.1947 - acc: 0.6965\n",
      "Epoch 113/250\n",
      " - 18s - loss: 0.1947 - acc: 0.6969\n",
      "Epoch 114/250\n",
      " - 18s - loss: 0.1943 - acc: 0.6972\n",
      "Epoch 115/250\n",
      " - 18s - loss: 0.1947 - acc: 0.6960\n",
      "Epoch 116/250\n",
      " - 19s - loss: 0.1940 - acc: 0.6978\n",
      "Epoch 117/250\n",
      " - 19s - loss: 0.1944 - acc: 0.6970\n",
      "Epoch 118/250\n",
      " - 19s - loss: 0.1942 - acc: 0.6972\n",
      "Epoch 119/250\n",
      " - 19s - loss: 0.1942 - acc: 0.6972\n",
      "Epoch 120/250\n",
      " - 19s - loss: 0.1938 - acc: 0.6974\n",
      "Epoch 121/250\n",
      " - 19s - loss: 0.1939 - acc: 0.6984\n",
      "Epoch 122/250\n",
      " - 18s - loss: 0.1937 - acc: 0.6983\n",
      "Epoch 123/250\n",
      " - 19s - loss: 0.1942 - acc: 0.6974\n",
      "Epoch 124/250\n",
      " - 19s - loss: 0.1936 - acc: 0.6988\n",
      "Epoch 125/250\n",
      " - 20s - loss: 0.1938 - acc: 0.6982\n",
      "Epoch 126/250\n",
      " - 18s - loss: 0.1934 - acc: 0.6986\n",
      "Epoch 127/250\n",
      " - 19s - loss: 0.1936 - acc: 0.6985\n",
      "Epoch 128/250\n",
      " - 18s - loss: 0.1932 - acc: 0.6990\n",
      "Epoch 129/250\n",
      " - 19s - loss: 0.1934 - acc: 0.6986\n",
      "Epoch 130/250\n",
      " - 19s - loss: 0.1934 - acc: 0.6988\n",
      "Epoch 131/250\n",
      " - 19s - loss: 0.1933 - acc: 0.6989\n",
      "Epoch 132/250\n",
      " - 19s - loss: 0.1934 - acc: 0.6982\n",
      "Epoch 133/250\n",
      " - 19s - loss: 0.1933 - acc: 0.6989\n",
      "Epoch 134/250\n",
      " - 19s - loss: 0.1931 - acc: 0.7000\n",
      "Epoch 135/250\n",
      " - 19s - loss: 0.1932 - acc: 0.6990\n",
      "Epoch 136/250\n",
      " - 19s - loss: 0.1929 - acc: 0.6999\n",
      "Epoch 137/250\n",
      " - 19s - loss: 0.1928 - acc: 0.7006\n",
      "Epoch 138/250\n",
      " - 19s - loss: 0.1928 - acc: 0.6998\n",
      "Epoch 139/250\n",
      " - 19s - loss: 0.1925 - acc: 0.7006\n",
      "Epoch 140/250\n",
      " - 19s - loss: 0.1925 - acc: 0.7011\n",
      "Epoch 141/250\n",
      " - 19s - loss: 0.1928 - acc: 0.7002\n",
      "Epoch 142/250\n",
      " - 19s - loss: 0.1931 - acc: 0.6993\n",
      "Epoch 143/250\n",
      " - 19s - loss: 0.1925 - acc: 0.7004\n",
      "Epoch 144/250\n",
      " - 19s - loss: 0.1923 - acc: 0.7016\n",
      "Epoch 145/250\n",
      " - 19s - loss: 0.1926 - acc: 0.7010\n",
      "Epoch 146/250\n",
      " - 19s - loss: 0.1924 - acc: 0.7015\n",
      "Epoch 147/250\n",
      " - 19s - loss: 0.1922 - acc: 0.7015\n",
      "Epoch 148/250\n",
      " - 19s - loss: 0.1922 - acc: 0.7017\n",
      "Epoch 149/250\n",
      " - 19s - loss: 0.1918 - acc: 0.7024\n",
      "Epoch 150/250\n",
      " - 19s - loss: 0.1921 - acc: 0.7013\n",
      "Epoch 151/250\n",
      " - 18s - loss: 0.1921 - acc: 0.7014\n",
      "Epoch 152/250\n",
      " - 19s - loss: 0.1919 - acc: 0.7014\n",
      "Epoch 153/250\n",
      " - 19s - loss: 0.1919 - acc: 0.7018\n",
      "Epoch 154/250\n",
      " - 19s - loss: 0.1919 - acc: 0.7021\n",
      "Epoch 155/250\n",
      " - 18s - loss: 0.1917 - acc: 0.7024\n",
      "Epoch 156/250\n",
      " - 19s - loss: 0.1916 - acc: 0.7029\n",
      "Epoch 157/250\n",
      " - 19s - loss: 0.1916 - acc: 0.7025\n",
      "Epoch 158/250\n",
      " - 19s - loss: 0.1913 - acc: 0.7031\n",
      "Epoch 159/250\n",
      " - 20s - loss: 0.1912 - acc: 0.7030\n",
      "Epoch 160/250\n",
      " - 19s - loss: 0.1914 - acc: 0.7033\n",
      "Epoch 161/250\n",
      " - 20s - loss: 0.1916 - acc: 0.7025\n",
      "Epoch 162/250\n",
      " - 19s - loss: 0.1911 - acc: 0.7037\n",
      "Epoch 163/250\n",
      " - 19s - loss: 0.1913 - acc: 0.7035\n",
      "Epoch 164/250\n",
      " - 19s - loss: 0.1910 - acc: 0.7039\n",
      "Epoch 165/250\n",
      " - 19s - loss: 0.1911 - acc: 0.7036\n",
      "Epoch 166/250\n",
      " - 18s - loss: 0.1910 - acc: 0.7036\n",
      "Epoch 167/250\n",
      " - 19s - loss: 0.1910 - acc: 0.7039\n",
      "Epoch 168/250\n",
      " - 19s - loss: 0.1914 - acc: 0.7030\n",
      "Epoch 169/250\n",
      " - 20s - loss: 0.1912 - acc: 0.7035\n",
      "Epoch 170/250\n",
      " - 19s - loss: 0.1907 - acc: 0.7041\n",
      "Epoch 171/250\n",
      " - 21s - loss: 0.1908 - acc: 0.7041\n",
      "Epoch 172/250\n",
      " - 19s - loss: 0.1909 - acc: 0.7042\n",
      "Epoch 173/250\n",
      " - 19s - loss: 0.1912 - acc: 0.7036\n",
      "Epoch 174/250\n",
      " - 20s - loss: 0.1908 - acc: 0.7040\n",
      "Epoch 175/250\n",
      " - 19s - loss: 0.1905 - acc: 0.7048\n",
      "Epoch 176/250\n",
      " - 19s - loss: 0.1905 - acc: 0.7050\n",
      "Epoch 177/250\n",
      " - 19s - loss: 0.1907 - acc: 0.7040\n",
      "Epoch 178/250\n",
      " - 20s - loss: 0.1906 - acc: 0.7048\n",
      "Epoch 179/250\n",
      " - 19s - loss: 0.1904 - acc: 0.7053\n",
      "Epoch 180/250\n",
      " - 19s - loss: 0.1906 - acc: 0.7048\n",
      "Epoch 181/250\n",
      " - 20s - loss: 0.1903 - acc: 0.7055\n",
      "Epoch 182/250\n",
      " - 19s - loss: 0.1903 - acc: 0.7051\n",
      "Epoch 183/250\n",
      " - 20s - loss: 0.1901 - acc: 0.7056\n",
      "Epoch 184/250\n",
      " - 19s - loss: 0.1902 - acc: 0.7054\n",
      "Epoch 185/250\n",
      " - 20s - loss: 0.1899 - acc: 0.7060\n",
      "Epoch 186/250\n",
      " - 19s - loss: 0.1901 - acc: 0.7060\n",
      "Epoch 187/250\n",
      " - 19s - loss: 0.1901 - acc: 0.7055\n",
      "Epoch 188/250\n",
      " - 18s - loss: 0.1900 - acc: 0.7055\n",
      "Epoch 189/250\n",
      " - 19s - loss: 0.1902 - acc: 0.7055\n",
      "Epoch 190/250\n",
      " - 19s - loss: 0.1898 - acc: 0.7065\n",
      "Epoch 191/250\n",
      " - 19s - loss: 0.1898 - acc: 0.7063\n",
      "Epoch 192/250\n",
      " - 18s - loss: 0.1894 - acc: 0.7069\n",
      "Epoch 193/250\n",
      " - 19s - loss: 0.1896 - acc: 0.7067\n",
      "Epoch 194/250\n",
      " - 18s - loss: 0.1901 - acc: 0.7050\n",
      "Epoch 195/250\n",
      " - 18s - loss: 0.1899 - acc: 0.7057\n",
      "Epoch 196/250\n",
      " - 19s - loss: 0.1896 - acc: 0.7064\n",
      "Epoch 197/250\n",
      " - 18s - loss: 0.1896 - acc: 0.7065\n",
      "Epoch 198/250\n",
      " - 19s - loss: 0.1893 - acc: 0.7071\n",
      "Epoch 199/250\n",
      " - 19s - loss: 0.1895 - acc: 0.7066\n",
      "Epoch 200/250\n",
      " - 19s - loss: 0.1891 - acc: 0.7073\n",
      "Epoch 201/250\n",
      " - 20s - loss: 0.1896 - acc: 0.7062\n",
      "Epoch 202/250\n",
      " - 19s - loss: 0.1896 - acc: 0.7068\n",
      "Epoch 203/250\n",
      " - 19s - loss: 0.1889 - acc: 0.7083\n",
      "Epoch 204/250\n",
      " - 19s - loss: 0.1895 - acc: 0.7064\n",
      "Epoch 205/250\n",
      " - 19s - loss: 0.1889 - acc: 0.7080\n",
      "Epoch 206/250\n",
      " - 19s - loss: 0.1889 - acc: 0.7078\n",
      "Epoch 207/250\n",
      " - 19s - loss: 0.1891 - acc: 0.7079\n",
      "Epoch 208/250\n",
      " - 19s - loss: 0.1890 - acc: 0.7073\n",
      "Epoch 209/250\n",
      " - 19s - loss: 0.1883 - acc: 0.7092\n",
      "Epoch 210/250\n",
      " - 18s - loss: 0.1886 - acc: 0.7088\n",
      "Epoch 211/250\n",
      " - 19s - loss: 0.1890 - acc: 0.7078\n",
      "Epoch 212/250\n",
      " - 19s - loss: 0.1890 - acc: 0.7077\n",
      "Epoch 213/250\n",
      " - 20s - loss: 0.1886 - acc: 0.7090\n",
      "Epoch 214/250\n",
      " - 20s - loss: 0.1891 - acc: 0.7075\n",
      "Epoch 215/250\n",
      " - 19s - loss: 0.1886 - acc: 0.7087\n",
      "Epoch 216/250\n",
      " - 19s - loss: 0.1887 - acc: 0.7085\n",
      "Epoch 217/250\n",
      " - 19s - loss: 0.1887 - acc: 0.7087\n",
      "Epoch 218/250\n",
      " - 19s - loss: 0.1883 - acc: 0.7095\n",
      "Epoch 219/250\n",
      " - 19s - loss: 0.1885 - acc: 0.7085\n",
      "Epoch 220/250\n",
      " - 19s - loss: 0.1885 - acc: 0.7093\n",
      "Epoch 221/250\n",
      " - 18s - loss: 0.1884 - acc: 0.7086\n",
      "Epoch 222/250\n",
      " - 19s - loss: 0.1884 - acc: 0.7091\n",
      "Epoch 223/250\n",
      " - 18s - loss: 0.1883 - acc: 0.7090\n",
      "Epoch 224/250\n",
      " - 18s - loss: 0.1883 - acc: 0.7089\n",
      "Epoch 225/250\n",
      " - 19s - loss: 0.1882 - acc: 0.7089\n",
      "Epoch 226/250\n",
      " - 19s - loss: 0.1879 - acc: 0.7101\n",
      "Epoch 227/250\n",
      " - 19s - loss: 0.1881 - acc: 0.7099\n",
      "Epoch 228/250\n",
      " - 20s - loss: 0.1882 - acc: 0.7091\n",
      "Epoch 229/250\n",
      " - 19s - loss: 0.1876 - acc: 0.7100\n",
      "Epoch 230/250\n",
      " - 19s - loss: 0.1879 - acc: 0.7104\n",
      "Epoch 231/250\n",
      " - 19s - loss: 0.1879 - acc: 0.7100\n",
      "Epoch 232/250\n",
      " - 18s - loss: 0.1878 - acc: 0.7100\n",
      "Epoch 233/250\n",
      " - 19s - loss: 0.1878 - acc: 0.7101\n",
      "Epoch 234/250\n",
      " - 19s - loss: 0.1877 - acc: 0.7102\n",
      "Epoch 235/250\n",
      " - 19s - loss: 0.1878 - acc: 0.7099\n",
      "Epoch 236/250\n",
      " - 18s - loss: 0.1879 - acc: 0.7099\n",
      "Epoch 237/250\n",
      " - 19s - loss: 0.1878 - acc: 0.7100\n",
      "Epoch 238/250\n",
      " - 19s - loss: 0.1877 - acc: 0.7100\n",
      "Epoch 239/250\n",
      " - 19s - loss: 0.1875 - acc: 0.7114\n",
      "Epoch 240/250\n",
      " - 19s - loss: 0.1874 - acc: 0.7113\n",
      "Epoch 241/250\n",
      " - 19s - loss: 0.1875 - acc: 0.7105\n",
      "Epoch 242/250\n",
      " - 19s - loss: 0.1874 - acc: 0.7112\n",
      "Epoch 243/250\n",
      " - 19s - loss: 0.1877 - acc: 0.7106\n",
      "Epoch 244/250\n",
      " - 19s - loss: 0.1874 - acc: 0.7106\n",
      "Epoch 245/250\n",
      " - 19s - loss: 0.1875 - acc: 0.7111\n",
      "Epoch 246/250\n",
      " - 19s - loss: 0.1875 - acc: 0.7106\n",
      "Epoch 247/250\n",
      " - 19s - loss: 0.1874 - acc: 0.7112\n",
      "Epoch 248/250\n",
      " - 19s - loss: 0.1870 - acc: 0.7114\n",
      "Epoch 249/250\n",
      " - 19s - loss: 0.1872 - acc: 0.7112\n",
      "Epoch 250/250\n",
      " - 19s - loss: 0.1873 - acc: 0.7115\n"
     ]
    }
   ],
   "source": [
    "d1 = 400\n",
    "d2 = 200\n",
    "d3 = 100\n",
    "d4 = 2\n",
    "\n",
    "# build 2 autoencoders\n",
    "a1 = 600\n",
    "a2 = 400\n",
    "a3 = 200\n",
    "a4 = 100\n",
    "\n",
    "# input1 = Input(shape = (x.shape[1],))\n",
    "# ae1 = Dense(a1, activation='relu',trainable=False)(input1)\n",
    "# ae2 = Dense(a2, activation='relu',trainable=False)(ae1)\n",
    "# ae3 = Dense(a3, activation='relu',trainable=False)(ae2)\n",
    "# out1 = Dense(a4, activation='relu',trainable=False)(ae3)\n",
    "\n",
    "# input2 = Input(shape = (x.shape[1],))\n",
    "# ae1 = Dense(a1, activation='relu',trainable=False)(input2)\n",
    "# ae2 = Dense(a2, activation='relu',trainable=False)(ae1)\n",
    "# ae3 = Dense(a3, activation='relu',trainable=False)(ae2)\n",
    "# out2 = Dense(a4, activation='relu',trainable=False)(ae3)\n",
    "\n",
    "# input1 = Input(shape=(x.shape[1],))\n",
    "# input2 = Input(shape=(x.shape[1],))\n",
    "\n",
    "input1 = Input(shape=(769,))\n",
    "input2 = Input(shape=(769,))\n",
    "\n",
    "batch_size=2560\n",
    "\n",
    "out1 = ae(input1)\n",
    "out2 = ae(input2)\n",
    "\n",
    "l1 = keras.layers.Concatenate()([out1, out2])\n",
    "l2 = Dense(d1, activation='elu')(l1)\n",
    "l3 = Dense(d2, activation='elu')(l2)\n",
    "l4 = Dense(d3, activation='elu')(l3)\n",
    "out = Dense(d4, activation='softmax')(l4)\n",
    "\n",
    "model = Model([input1, input2], out)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
    "\n",
    "def myGenerator():\n",
    "    while 1:\n",
    "        x, y = shuffle(1000000)\n",
    "        for i in range(1000000//batch_size):\n",
    "            yield [[x[0][i*batch_size:(i+1)*batch_size], x[1][i*batch_size:(i+1)*batch_size]],\n",
    "                   y[i*batch_size:(i+1)*batch_size,:]]\n",
    "\n",
    "hist = model.fit_generator(myGenerator(), \n",
    "                    steps_per_epoch = 1000000//batch_size, \n",
    "                    epochs = 250,\n",
    "                    verbose = 2)\n",
    "#                     callbacks = [keras.callbacks.EarlyStopping(patience=20)],\n",
    "#                    validation_data = (xxt, yyt))\n",
    "# for i in range(1000):\n",
    "#     xx, y = shuffle(500)\n",
    "#     print(i)\n",
    "#     model.fit(xx, y, epochs=i, initial_epoch=i-1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "x1 = np.random.rand(200000,769)\n",
    "x2 = np.random.rand(200000,769) # about 2gb of ram\n",
    "with h5py.File('chess_data.hdf5','w') as f:\n",
    "    dset = f.create_dataset(\"x1\", x1.shape, chunks=(2500, x1.shape[1]), compression=\"gzip\")\n",
    "    dset[:] = x1\n",
    "    dset = f.create_dataset(\"x2\", x2.shape, chunks=(2500, x2.shape[1]), compression=\"gzip\")\n",
    "    dset[:] = x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset memory:\n",
    "x1 = 0\n",
    "x2 = 0\n",
    "# randomly shuffle the data, getting a random permutation, but without needing to hold the array twice:\n",
    "def low_mem_shuffle(n, filename):\n",
    "    # n number of random samples. Maybe it will automatically load chunks if I just randomly permute the data:\n",
    "    out = []\n",
    "    with h5py.File(filename+'.hdf5','r') as f:\n",
    "        x1 = f['white'][:]\n",
    "        x2 = f['black'][:]\n",
    "        ind1 = sorted(np.random.permutation(range(x1.shape[0]))[:n])\n",
    "        ind2 = sorted(np.random.permutation(range(x2.shape[0]))[:n])\n",
    "        xx1 = np.random.permutation(x1[ind1])\n",
    "        xx2 = np.random.permutation(x2[ind2])\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(xx1.shape[0]):\n",
    "        chance = np.random.rand()\n",
    "        if chance > 0.5:\n",
    "            x.append([xx1[i], xx2[i]])\n",
    "            y.append([0,1])\n",
    "        else:\n",
    "            x.append([xx2[i], xx1[i]])\n",
    "            y.append([1,0])\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return [x[:,0], x[:,1]], y\n",
    "a, b = low_mem_shuffle(50000,'raw_chess_data')\n",
    "#         out.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\_hl\\selections.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(shape, args, dsid)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-03380fd54370>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a, b = low_mem_shuffle(50000,'raw_chess_data')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2129\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2130\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2131\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2132\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-62>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1096\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 \u001b[0mtime_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-58876b5e150e>\u001b[0m in \u001b[0;36mlow_mem_shuffle\u001b[1;34m(n, filename)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mind2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mxx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mxx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;31m# Perform the dataspace selection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0mselection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdsid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnselect\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\_hl\\selections.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(shape, args, dsid)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0msel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFancySelection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                 \u001b[0msel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0msel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\_hl\\selections.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_handle_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_hyperslab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mh5s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSELECT_OR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;31m# Final shape excludes scalars, except where\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a, b = low_mem_shuffle(50000,'raw_chess_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.498\n"
     ]
    }
   ],
   "source": [
    "print(b[:,0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
